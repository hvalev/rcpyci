# rcpyci üå∂Ô∏è
rcpyci (/…ër Ààspa…™si/) is a reverse correlation classification images python implementation closely mirroring the R package [rcicr](https://github.com/rdotsch/rcicr/). rcpyci can do everything the R package can, while much easier to install thanks to the extensive python package ecosystem and much faster due to numpy's broadcasting and parallel processing. rcpyci supports the following tasks:
- generate stimulus images for 2afc tasks
- create classification images from 2afc tasks split by participant or condition
- compute zmaps on classification images or parameter space
- cache intermediary results for further analyses
- generic way to add additional pipelines for computing anything on the ci- or parameter-space

## How to generate stimuli for a 2IFC task
To generate stimuli for a 2IFC task, the following snippet would be enough for the basic use case. Be mindful that this uses a base_face image which is included in the repository under `tests`.
```
from rcpyci.interface import setup_experiment
base_face_path = "./base_face.jpg"
setup_experiment(base_face_path)
```
`setup_experiment` further exposes the parameters `n_trials`, `n_scales`, `sigma`, `noise_type`, `seed` to allow the user to tune how many trials will be generated as well as tweak the noise-generation parameters and select the seed used for randomization. Check the documentation of `setup_experiment` for more information on how they can be tuned.
__NOTE__: Setup experiment will leave a folder with some basic files that contain some information about how the stimuli were generated. It is important to keep this file, as the noise generation parameters and seed value will be used to re-generate the same stimuli when generating the classification images.

## How to analyze my data
In order to easily analyze your data, you need to prepare your results to be of a specific format. Afterwards you can use the convenience function in `interface`. In sum, your dataset needs to be loaded as a single dataframe with 5 columns: `idx`, `condition`, `participant_id`, `stimulus_id` and `responses`. The `idx` column is a unique identifier for each row, `condition` is the condition that was used for each trial. In case a single condition is investigated, then there will be a single value. `participant_id` is a unique identifier for each participant in your experiment. `stimulus_id` is a unique identifier for each stimulus (pair of original and inverted images generated by setup_experiment) and `responses` is the response that was given by each participant for each stimulus (the choice between the original or inverted image). If a participant has selected the original image, then the response is `1`, otherwise it will be `-1`. You can generate a sample dataframe and try out the analysis function by running the code-snippet below:

```
from rcpyci.interface import analyze_data
from rcpyci.utils import create_test_data, verify_data

sample_data = create_test_data(n_trials=500)
verify_data(sample_data)
base_face_path = "./base_face.jpg"
analyze_data(sample_data, base_face_path, n_trials=500)
```
In reality, you will instead load your own dataframe and pass the arguments used in generating the stimulus image to the `analyze_data` function as well as any additional tweaks you want to apply. For more information, check the method signature. Be mindful that the library makes use of `joblib` to parallelize computation and spawns `n_jobs` python processes to handle each data 'split' in the dataframe. If you're using `analyze_data` from the `interface` namespace, that would be split by participant and condition. As such you should make sure that you have enough memory for the number of concurrent processes. Typically if you have around 20GB RAM, you could use around 6-8 concurrent jobs. After the initial workloads have been assigned the memory usage normalizes, since the computation is not stuck simultaneously on memory intensive tasks (such as computing zmaps on the parameter space). The easiest way to find the optimal number of concurrent jobs is by trial and error.


## What is in this package
The package includes 4 main namespaces:
`core` contains all the core functionality for creating the stimulus images and computing the classification images. The functions within the `core` namespace work directly with arrays representing the images or various intermediary results. `interface` is a convenience namespace which contains functions used by the user to easily create the stimuli or compute the classification images. `pipelines` is a namespace which defines how the classification images are calculated as well as functions for computing zmaps on the CI space or the stimulus parameter space. `im_ops` is a namespace which defines operations on image arrays and `utils` contains some helper functions.
__NOTE__: `infoval` is a port of the `infoval` functionality from the originl package, but it is untested.

### Custom pipelines for post-processing CIs
You can write your own pipelines for computing further information on classification images and a number of internals exposed to the pipelines. 

An example speaks a thousand words:
```
from rcpyci.interface import analyze_data
from rcpyci.utils import create_test_data, verify_data, cache_as_numpy

sample_data = create_test_data(n_trials=500)
verify_data(sample_data)
base_face_path = "./base_face.jpg"

sample_pipe_generator_kwargs = {
    'addition': 1000
}

def sample_pipe_generator(seed, addition):
    return {'modified_seed': seed + addition}

sample_pipe_receiver_kwargs = {
    'use_cache': True,
    'save_folder': 'sample'
}

@cache_as_numpy
def sample_pipe_receiver(modified_seed, cache=None):
    return {'modified_seed': modified_seed}

pipelines = [
    (sample_pipe_generator, sample_pipe_generator_kwargs),
    (sample_pipe_receiver, sample_pipe_receiver_kwargs)
]

analyze_data(sample_data, base_face_path, pipelines=pipelines, n_trials=500)
```
In this example we create a pipeline that adds 1000 to the seed of each trial. We then cache the results as numpy arrays and save them in the `sample` folder. In the generator, we use the `seed` variable as well as a pipeline-specific `addition` keyword argument. We then return a variable called `modified_seed` which is used by second, receiver pipeline. There, we use caching and store this variable as a numpy array.

Of course, this is meaningless, but it provides a skeleton code and example of how to create your own pipelines, how to use caching, how to use internal parameters involved in the creation of the stimulus images, as well as pipeline-specific kwargs, and how to pass variables between pipelines.

### How does this compare to the R package
The core part of the package is intended to be equivalent to R's implementation. However, as this package relies on numpy for most of the numeric computations and by leveraging numpy's broadcasting for parallelizing operations, this has led to dramatic speed increases and stable memory use. In addition, the architecture of the package allows people with little coding experince to be able to use the package by utilizing the user-friendly `interface` namespace, while also allowing seasoned users to directly access some lower-level interfaces and write their own image processing pipelines. Also, by seeding the random number generator for creating the noise pattern, the stimulus images can be reproduced provided the same seed is used. Finally, the package allows some degree of interoperability with the R package. More on that in the next section.

## Compatibility to R's rcicr
The implementation should produce the same results between this one and R's implementations with a few considerations to be made. First, there are differences in how `R` and `python's` `numpy` and `random` packages generate random numbers. This results in the underlying generated parameter distribution used in creating the stimulus material is not interchangeable using the same seed. The margin of errors being introduced by rounding errors and differences in implementation, the biggest being at computing the cis of a participant at `~0.0005`. The complete test suite can be ran by running the `run_tests.sh` script from inside the `ref` folder in this repository. More info [here](ref/README.md).

## How to use R stimuli in rcpyci
If you have experiment data created with the R `rcicr` package, you can still use `rcpyci` to process your data. As the random number generation approaches between python and R are not interchangeable, the parameter space generated by `rcicr` needs to be exported. After that, you can process your data using the faster or more extensible `rcpyci`. 

The easiest way to export the parameter space is to use an environment which has a functional R environment as well as the `rpy2` python package. The easiest way is to use the docker container used in the tests since it already has both.
You can start the container, add the correct parts to the environment variables and start the correct python interpreter as follows:
```
docker run -it -w / -v ./data:/data hvalev/rcpyci  /bin/bash
export R_HOME=/usr/local/lib/R && export LD_LIBRARY_PATH=/usr/local/lib/R/lib:/usr/local/lib/R/
/pyrcicr/bin/python3
```
Make sure that you have your RData file created by `rcicr` in the `./data` folder. Afterwards you need to load and convert to to a numpy array like this:
```
import rpy2.robjects as robjects
import numpy as np
robjects.r['load']("/data/test.RData")
z = np.array(robjects.conversion.rpy2py(robjects.r['stimuli_params']))
z.shape
```
In my case, the number of trials were 500, hence the shape of the reshaped array below. Feel free to adjust the number below to the number of trials used in your own experiment. Additionally, you can double-check that the max and min values in the converted array are within the (-1,1) interval as one would expect.
```
t = z.reshape(500,4092)
np.save('/data/stimulus.npy', t)
t.max()
t.min()
```
In fact, the last 2 commands should yield approx. 0.99(9) and -0.99(9) respectively.

With the stimulus.npy file in place you can analyze your data with `rcpyci` by feeding the contents in the `stimulus_param` variable in the `analyze_data` function in `rcpyci.interface`. This would disable re-generating the parameter space using the provided seed value and use this sideloaded parameter space instead. You must make sure that you provide matching parameters for `n_scales` and `n_trials` so that the patches and patch indices are generated with the correct array dimensions.
