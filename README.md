# rcpyci üå∂Ô∏è
r-cpyci (/…ër Ààspa…™si/) is a python implementation of the R package [rcicr](https://github.com/rdotsch/rcicr/) to perform research using the reverse correlation classification images. The package allows to generate stimulus images for experiments and also generate cis for participants or conditions.

## How to generate stimuli for a 2IFC task
To generate stimuli for a 2IFC task, the following snippet would be enough for the basic use case. Be mindful that this uses a base_face image which is included in the repository under `tests`.
```
from rcpyci.interface import setup_experiment
base_face_path = "./base_face.jpg"
setup_experiment(base_face_path)
```
`setup_experiment` further exposes the parameters `n_trials`, `n_scales`, `sigma`, `noise_type`, `seed` to allow the user to tune how many trials will be generated as well as tweak the noise-generation parameters and select the seed used for randomization. Check the documentation of `setup_experiment` for more information on how they can be tuned.
__NOTE__: Setup experiment will leave a folder with some basic files that contain some information about how the stimuli were generated. It is important to keep this file, as the noise generation parameters and seed value will be used to re-generate the same stimuli when generating the classification images.

## How to analyze my data
In order to easily analyze your data, you need to prepare your results to be of a specific format. Afterwards you can use the convenience function in `interface`. In sum, your dataset needs to be loaded as a single dataframe with 5 columns: `idx`, `condition`, `participant_id`, `stimulus_id` and `responses`. The `idx` column is a unique identifier for each row, `condition` is the condition that was used for each trial. In case a single condition is investigated, then there will be a single value. `participant_id` is a unique identifier for each participant in your experiment. `stimulus_id` is a unique identifier for each stimulus (pair of original and inverted images generated by setup_experiment) and `responses` is the response that was given by each participant for each stimulus (the choice between the original or inverted image). If a participant has selected the original image, then the response is `1`, otherwise it will be `-1`. You can generate a sample dataframe and try out the analysis function by running the code-snippet below:

```
from rcpyci.interface import analyze_data
from rcpyci.utils import create_test_data, verify_data

sample_data = create_test_data(n_trials=500)
verify_data(sample_data)
base_face_path = "./base_face.jpg"
analyze_data(sample_data, base_face_path, n_trials=500)
```
In reality, you will instead load your own dataframe and pass the arguments used in generating the stimulus image to the `analyze_data` function as well as any additional tweaks you want to apply. For more information, check the method signature

## What is in this package
The package includes 4 main namespaces:
`core` contains all the core functionality for creating the stimulus images and computing the classification images. The functions within the `core` namespace work directly with arrays representing the images or various intermediary results. `interface` is a convenience namespace which contains functions used by the user to easily create the stimuli or compute the classification images. `pipelines` is a namespace which defines how the classification images are calculated as well as functions for computing zmaps on the CI space or the stimulus parameter space. `im_ops` is a namespace which defines operations on image arrays and `utils` contains some helper functions.
__NOTE__: `infoval` is a port of the `infoval` functionality from the originl package, but it is untested.

### Custom pipelines for processing CIs or zmaps
You can write your own pipeline for computing zmaps. In pipelines.py you can find skeleton code for a pipeline, where the signature of the function includes all default keyword arguments passed from the compute_ci_and_zmaps internals to avoid repeating the same computations when analyzing your data. In addition, further keyword arguments can be provided by passing a dictionary with the key-value pairs.  

### How does this compare to the R package
The core part of the package is intended to be equivalent to R's implementation. However, as this package relies on numpy for most of the numeric computations and by leveraging numpy's broadcasting for parallelizing operations, this has led to dramatic speed increases and stable memory use. In addition, the architecture of the package allows people with little coding experince to be able to use the package by utilizing the user-friendly `interface` namespace, while also allowing seasoned users to directly access some lower-level interfaces and write their own image processing pipelines. Also, by seeding the random number generator for creating the noise pattern, the stimulus images can be reproduced provided the same seed is used. Finally, the package allows some degree of interoperability with the R package. More on that in the next section.

## Compatibility to R's rcicr
The implementation should produce the same results between this one and R's implementations with a few considerations to be made. First, there are differences in how `R` and `python's` `numpy` and `random` packages generate random numbers. This results in the underlying generated parameter distribution used in creating the stimulus material is not interchangeable using the same seed. The margin of errors being introduced by rounding errors and differences in implementation, the biggest being at computing the cis of a participant at `~0.0005`. The complete test suite can be ran by running the `run_tests.sh` script from inside the `ref` folder in this repository. More info [here](ref/README.md).

## How to use R stimuli in rcpyci
If you have experiment data created with the R `rcicr` package, you can still use `rcpyci` to process your data. As the random number generation approaches between python and R are not interchangeable, the parameter space generated by `rcicr` needs to be exported. After that, you can process your data using the faster or more extensible `rcpyci`. 

The easiest way to export the parameter space is to use an environment which has a functional R environment as well as the `rpy2` python package. The easiest way is to use the docker container used in the tests since it already has both.
You can start the container, add the correct parts to the environment variables and start the correct python interpreter as follows:
```
docker run -it -w / -v ./data:/data hvalev/rcpyci  /bin/bash
export R_HOME=/usr/local/lib/R && export LD_LIBRARY_PATH=/usr/local/lib/R/lib:/usr/local/lib/R/
/pyrcicr/bin/python3
```
Make sure that you have your RData file created by `rcicr` in the `./data` folder. Afterwards you need to load and convert to to a numpy array like this:
```
import rpy2.robjects as robjects
import numpy as np
robjects.r['load']("/data/test.RData")
z = np.array(robjects.conversion.rpy2py(robjects.r['stimuli_params']))
z.shape
```
In my case, the number of trials were 500, hence the shape of the reshaped array below. Feel free to adjust the number below to the number of trials used in your own experiment. Additionally, you can double-check that the max and min values in the converted array are within the (-1,1) interval as one would expect.
```
t = z.reshape(500,4092)
np.save('/data/stimulus.npy', t)
t.max()
t.min()
```
In fact, the last 2 commands should yield approx. 0.99(9) and -0.99(9) respectively.

With the stimulus.npy file in place you can analyze your data with `rcpyci` by feeding the contents in the `stimulus_param` variable in the `analyze_data` function in `rcpyci.interface`. This would disable re-generating the parameter space using the provided seed value and use this sideloaded parameter space instead. You must make sure that you provide matching parameters for `n_scales` and `n_trials` so that the patches and patch indices are generated with the correct array dimensions.
